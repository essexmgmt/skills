{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt >> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract Transform and Loads skills data\n",
    "\"\"\"\n",
    "import os\n",
    "import pandas as pd\n",
    "import zipfile as zf\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class ETLSkills:\n",
    "    \"\"\"\n",
    "    Returns one dataframe of aggregated skills from an archived zip file of xlsx skills\n",
    "    \"\"\"\n",
    "    skills_archive = \"skills_essex.zip\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.dfs = []\n",
    "\n",
    "    def extract(self):\n",
    "        skills_archive = self.skills_archive\n",
    "        for fname in os.listdir(f\"temp_extract_{skills_archive}\"):\n",
    "            if fname[-4:] == \"xlsx\":\n",
    "                print(f\"Reading excel {fname} ...\")\n",
    "                self.dfs.append(pd.read_excel(f\"temp_extract_{skills_archive}/\"+fname))\n",
    "        return pd.concat(self.dfs, sort=True)\n",
    "\n",
    "    def write_to_file(self, dfs):\n",
    "        tmstmp = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "        writer = pd.ExcelWriter(f'./aggregate_skills_{tmstmp}.xlsx', engine='xlsxwriter')\n",
    "        sheet_names = ['sheet1']\n",
    "        for df, sheet_name in zip(dfs, sheet_names):\n",
    "            df.to_excel(writer, sheet_name=sheet_name)\n",
    "        writer.save()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    etl = ETLSkills()\n",
    "    df = etl.extract()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregates non-empty cells of skills\n",
    "skills = []\n",
    "for index, row in df.iterrows():\n",
    "    for i in range(0, len(row)):\n",
    "        if not pd.isna(row[i]):\n",
    "            skills.append(row[i])\n",
    "len(skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and extract entities\n",
    "from nltk import word_tokenize\n",
    "tokens = word_tokenize(\" \".join(skills))\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out stop words, punctuation, qualifliers, and quantifiers\n",
    "from nltk.corpus import stopwords \n",
    "stop_words = set(stopwords.words('english'))\n",
    "articles = ['a', 'the', 'an', 'I', ]\n",
    "directions = ['details','list','level','rows','skills']\n",
    "qualifiers = ['advanced', 'intermediate', 'beginner', 'exploratory', 'expert', 'certified', 'experience', 'basic', 'good', 'some', 'fluent']\n",
    "quantifiers = [str(n) for n in range(0,50)]\n",
    "not_in_context = ['yr', 'yrs', 'years', 'e.g', 'w/', \"`s\", ]\n",
    "\n",
    "filters = [stop_words, directions, qualifiers, quantifiers, not_in_context] \n",
    "def filter_tokens(filters):\n",
    "    filtered_tokens = tokens\n",
    "    for token_filter in filters:\n",
    "        filtered_tokens = [w for w in filtered_tokens if not w.lower() in token_filter and w.isalpha()] \n",
    "    return filtered_tokens\n",
    "\n",
    "filtered_tokens = filter_tokens(filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parts of speech tagging\n",
    "post_tokens = nltk.pos_tag(filtered_tokens)\n",
    "entities = []\n",
    "verbs = []\n",
    "concepts = []\n",
    "remainder = []\n",
    "for token in post_tokens:\n",
    "    # entity extraction\n",
    "    if token[1] in ['NN', 'NNP', 'NNPS', 'NNS']:\n",
    "        entities.append(token[0])\n",
    "    elif token[1] in ['JJ']:\n",
    "        concepts.append(token[0])\n",
    "    elif token[1] in ['VBD', 'VBG', 'VBN', 'VBP']:\n",
    "        verbs.append(token[0])\n",
    "    else:\n",
    "        remainder.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_entities = \",\".join(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Future analysis TODO:\n",
    "# tf-idf (skill frequency : # technical entities with that skill describes a skill area)\n",
    "# LSA / LDA clustering of skills to identify similarity between skills and transferrability\n",
    "    # if you know A, recommend learning a similar skill B\n",
    "# identify qualifiers to associate as rank weights for each skills polarity\n",
    "# entity classification by specified categories [FRONTEND, BACKEND, DATABASES, CLOUD, BIOLOGY, GENETICS, CHEMISTRY, NLP, ML, etc]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
